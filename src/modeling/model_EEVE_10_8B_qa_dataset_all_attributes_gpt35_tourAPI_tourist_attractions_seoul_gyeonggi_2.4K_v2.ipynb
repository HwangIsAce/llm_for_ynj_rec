{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tmp = pd.read_excel('/home/hwang/projects/yanolja/data/dataset/tourAPI/touristAttractions.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ëª…ì¹­</th>\n",
       "      <th>ìš°í¸ë²ˆí˜¸</th>\n",
       "      <th>ê´€ë¦¬ì</th>\n",
       "      <th>ì „í™”ë²ˆí˜¸</th>\n",
       "      <th>ì£¼ì†Œ</th>\n",
       "      <th>ìœ„ë„</th>\n",
       "      <th>ê²½ë„</th>\n",
       "      <th>ê°œìš”</th>\n",
       "      <th>ìœ ì‚°êµ¬ë¶„</th>\n",
       "      <th>ë¬¸ì˜ ë° ì•ˆë‚´</th>\n",
       "      <th>...</th>\n",
       "      <th>ì²´í—˜ì•ˆë‚´</th>\n",
       "      <th>ì²´í—˜ê°€ëŠ¥ì—°ë ¹</th>\n",
       "      <th>ìˆ˜ìš©ì¸ì›</th>\n",
       "      <th>ì´ìš©ì‹œê¸°</th>\n",
       "      <th>ì´ìš©ì‹œê°„</th>\n",
       "      <th>ì£¼ì°¨ì‹œì„¤</th>\n",
       "      <th>ìœ ëª¨ì°¨ ëŒ€ì—¬ ì—¬ë¶€</th>\n",
       "      <th>ì• ì™„ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥ ì—¬ë¶€</th>\n",
       "      <th>ì‹ ìš©ì¹´ë“œ ê°€ëŠ¥ ì—¬ë¶€</th>\n",
       "      <th>ìƒì„¸ì •ë³´</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ìˆ˜ì„±ëª» ìœ ì›ì§€</td>\n",
       "      <td>42202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ëŒ€êµ¬ê´‘ì—­ì‹œ ìˆ˜ì„±êµ¬ ìš©í•™ë¡œ 35-5</td>\n",
       "      <td>35.826842</td>\n",
       "      <td>128.612873</td>\n",
       "      <td>ìˆ˜ì„±ìœ ì›ì§€ëŠ” ë²”ë¬¼ë™ ìš©ì§€ë´‰(629ï½)ì—ì„œ ë¶ì„œë¶€ë¡œ ë»—ì–´ ë‚´ë¦° ì¤„ê¸°ì˜ í•˜ë¶€ì— ìœ„ì¹˜í•˜ì—¬...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ìˆ˜ì„±ìœ ì›ì§€ 053-666-2863&lt;br/&gt;\\nìˆ˜ì„±ëª» ê´€ê´‘ì•ˆë‚´ì†Œ 053-761-0645</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ì „ì—°ë ¹</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00:00~24:00&lt;br&gt;\\r\\nâ€» ìˆ˜ì„± ë¯¸ë””ì–´ì•„íŠ¸ ìŒì•…ë¶„ìˆ˜ ìš´ì˜ì‹œê°„ì€ í™ˆí˜ì´ì§€ ...</td>\n",
       "      <td>ê°€ëŠ¥</td>\n",
       "      <td>ì—†ìŒ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ì´ìš©ê°€ëŠ¥ì‹œì„¤:- ìˆ˜ì„±ëª», ì˜¤ë¦¬ë°°&amp;ìœ ëŒì„ , ìˆ˜ë³€ë°í¬ë¡œë“œ, í¬ì¼“ë¬´ëŒ€, ê´€ê´‘ì•ˆë‚´ì†Œ ë¶ì¹´í˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë™ë½ê³µì›</td>\n",
       "      <td>39392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê²½ìƒë¶ë„ êµ¬ë¯¸ì‹œ 3ê³µë‹¨1ë¡œ 191(ì§„í‰ë™)</td>\n",
       "      <td>36.097116</td>\n",
       "      <td>128.401951</td>\n",
       "      <td>ë‚™ë™ê°•ì„ ë”°ë¼ êµ¬ë¯¸ëŒ€êµ ì•„ë˜ ì„ìˆ˜ë™ì—ì„œ ì¹ ê³¡êµ° ì„ì ì ì¤‘ë¦¬ê¹Œì§€ ì´ë¥´ëŠ” ìˆ˜ë³€í˜• ë„ì‹œê³µ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>054-480-4612</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ìƒì‹œê°œë°©</td>\n",
       "      <td>ìˆìŒ(ì•½ ì†Œí˜• 946ëŒ€)</td>\n",
       "      <td>ì—†ìŒ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ì£¼ì°¨ìš”ê¸ˆ:ë¬´ë£Œ\\nì… ì¥ ë£Œ:ë¬´ë£Œ\\ní™”ì¥ì‹¤:ìˆìŒ(ë‚¨/ë…€ êµ¬ë¶„)\\nì´ìš©ê°€ëŠ¥ì‹œì„¤:* ì²´ìœ¡...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ëª…ì¹­   ìš°í¸ë²ˆí˜¸  ê´€ë¦¬ì ì „í™”ë²ˆí˜¸                       ì£¼ì†Œ         ìœ„ë„          ê²½ë„  \\\n",
       "0  ìˆ˜ì„±ëª» ìœ ì›ì§€  42202  NaN  NaN       ëŒ€êµ¬ê´‘ì—­ì‹œ ìˆ˜ì„±êµ¬ ìš©í•™ë¡œ 35-5  35.826842  128.612873   \n",
       "1     ë™ë½ê³µì›  39392  NaN  NaN  ê²½ìƒë¶ë„ êµ¬ë¯¸ì‹œ 3ê³µë‹¨1ë¡œ 191(ì§„í‰ë™)  36.097116  128.401951   \n",
       "\n",
       "                                                  ê°œìš”  ìœ ì‚°êµ¬ë¶„  \\\n",
       "0  ìˆ˜ì„±ìœ ì›ì§€ëŠ” ë²”ë¬¼ë™ ìš©ì§€ë´‰(629ï½)ì—ì„œ ë¶ì„œë¶€ë¡œ ë»—ì–´ ë‚´ë¦° ì¤„ê¸°ì˜ í•˜ë¶€ì— ìœ„ì¹˜í•˜ì—¬...   NaN   \n",
       "1  ë‚™ë™ê°•ì„ ë”°ë¼ êµ¬ë¯¸ëŒ€êµ ì•„ë˜ ì„ìˆ˜ë™ì—ì„œ ì¹ ê³¡êµ° ì„ì ì ì¤‘ë¦¬ê¹Œì§€ ì´ë¥´ëŠ” ìˆ˜ë³€í˜• ë„ì‹œê³µ...   NaN   \n",
       "\n",
       "                                           ë¬¸ì˜ ë° ì•ˆë‚´  ... ì²´í—˜ì•ˆë‚´ ì²´í—˜ê°€ëŠ¥ì—°ë ¹ ìˆ˜ìš©ì¸ì› ì´ìš©ì‹œê¸°  \\\n",
       "0  ìˆ˜ì„±ìœ ì›ì§€ 053-666-2863<br/>\\nìˆ˜ì„±ëª» ê´€ê´‘ì•ˆë‚´ì†Œ 053-761-0645  ...  NaN    ì „ì—°ë ¹  NaN  NaN   \n",
       "1                                     054-480-4612  ...  NaN    NaN  NaN  NaN   \n",
       "\n",
       "                                                ì´ìš©ì‹œê°„           ì£¼ì°¨ì‹œì„¤ ìœ ëª¨ì°¨ ëŒ€ì—¬ ì—¬ë¶€  \\\n",
       "0  00:00~24:00<br>\\r\\nâ€» ìˆ˜ì„± ë¯¸ë””ì–´ì•„íŠ¸ ìŒì•…ë¶„ìˆ˜ ìš´ì˜ì‹œê°„ì€ í™ˆí˜ì´ì§€ ...             ê°€ëŠ¥        ì—†ìŒ   \n",
       "1                                               ìƒì‹œê°œë°©  ìˆìŒ(ì•½ ì†Œí˜• 946ëŒ€)        ì—†ìŒ   \n",
       "\n",
       "  ì• ì™„ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥ ì—¬ë¶€ ì‹ ìš©ì¹´ë“œ ê°€ëŠ¥ ì—¬ë¶€                                               ìƒì„¸ì •ë³´  \n",
       "0           NaN        NaN  ì´ìš©ê°€ëŠ¥ì‹œì„¤:- ìˆ˜ì„±ëª», ì˜¤ë¦¬ë°°&ìœ ëŒì„ , ìˆ˜ë³€ë°í¬ë¡œë“œ, í¬ì¼“ë¬´ëŒ€, ê´€ê´‘ì•ˆë‚´ì†Œ ë¶ì¹´í˜...  \n",
       "1           NaN        NaN  ì£¼ì°¨ìš”ê¸ˆ:ë¬´ë£Œ\\nì… ì¥ ë£Œ:ë¬´ë£Œ\\ní™”ì¥ì‹¤:ìˆìŒ(ë‚¨/ë…€ êµ¬ë¶„)\\nì´ìš©ê°€ëŠ¥ì‹œì„¤:* ì²´ìœ¡...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ëª…ì¹­</th>\n",
       "      <th>ìš°í¸ë²ˆí˜¸</th>\n",
       "      <th>ê´€ë¦¬ì</th>\n",
       "      <th>ì „í™”ë²ˆí˜¸</th>\n",
       "      <th>ì£¼ì†Œ</th>\n",
       "      <th>ìœ„ë„</th>\n",
       "      <th>ê²½ë„</th>\n",
       "      <th>ê°œìš”</th>\n",
       "      <th>ìœ ì‚°êµ¬ë¶„</th>\n",
       "      <th>ë¬¸ì˜ ë° ì•ˆë‚´</th>\n",
       "      <th>...</th>\n",
       "      <th>ì²´í—˜ì•ˆë‚´</th>\n",
       "      <th>ì²´í—˜ê°€ëŠ¥ì—°ë ¹</th>\n",
       "      <th>ìˆ˜ìš©ì¸ì›</th>\n",
       "      <th>ì´ìš©ì‹œê¸°</th>\n",
       "      <th>ì´ìš©ì‹œê°„</th>\n",
       "      <th>ì£¼ì°¨ì‹œì„¤</th>\n",
       "      <th>ìœ ëª¨ì°¨ ëŒ€ì—¬ ì—¬ë¶€</th>\n",
       "      <th>ì• ì™„ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥ ì—¬ë¶€</th>\n",
       "      <th>ì‹ ìš©ì¹´ë“œ ê°€ëŠ¥ ì—¬ë¶€</th>\n",
       "      <th>ìƒì„¸ì •ë³´</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>íƒœí’ì „ë§ëŒ€</td>\n",
       "      <td>11004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ê²½ê¸°ë„ ì—°ì²œêµ° ì¤‘ë©´ íš¡ì‚°ë¦¬</td>\n",
       "      <td>38.125122</td>\n",
       "      <td>126.974366</td>\n",
       "      <td>íƒœí’ì „ë§ëŒ€ëŠ” ì²œí•˜ë¬´ì  íƒœí’ë¶€ëŒ€ì—ì„œ 1991ë…„ 12ì›” 3ì¼ ê±´ë¦½í•œ ê²ƒìœ¼ë¡œ, ì„œìš¸ì—ì„œ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>031-839-2147</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09:00~16:00</td>\n",
       "      <td>ê°€ëŠ¥ (ë¬´ë£Œ)</td>\n",
       "      <td>ì—†ìŒ</td>\n",
       "      <td>ë¶ˆê°€</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ì´ìš©ê°€ëŠ¥ì‹œì„¤:ì „ë§ëŒ€ ì£¼ë³€ì˜ ê¸°ë…ë¹„ ë° ì°¸ì „íƒ‘, ì¥ë³‘ë“¤ì˜ ì¢…êµ ì§‘íšŒì¥ì†Œì¸ êµíšŒÂ·ì„±ë‹¹Â·...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ëª…ì¹­   ìš°í¸ë²ˆí˜¸  ê´€ë¦¬ì ì „í™”ë²ˆí˜¸              ì£¼ì†Œ         ìœ„ë„          ê²½ë„  \\\n",
       "10690  íƒœí’ì „ë§ëŒ€  11004  NaN  NaN  ê²½ê¸°ë„ ì—°ì²œêµ° ì¤‘ë©´ íš¡ì‚°ë¦¬  38.125122  126.974366   \n",
       "\n",
       "                                                      ê°œìš”  ìœ ì‚°êµ¬ë¶„       ë¬¸ì˜ ë° ì•ˆë‚´  \\\n",
       "10690  íƒœí’ì „ë§ëŒ€ëŠ” ì²œí•˜ë¬´ì  íƒœí’ë¶€ëŒ€ì—ì„œ 1991ë…„ 12ì›” 3ì¼ ê±´ë¦½í•œ ê²ƒìœ¼ë¡œ, ì„œìš¸ì—ì„œ ...   NaN  031-839-2147   \n",
       "\n",
       "       ... ì²´í—˜ì•ˆë‚´ ì²´í—˜ê°€ëŠ¥ì—°ë ¹ ìˆ˜ìš©ì¸ì› ì´ìš©ì‹œê¸°         ì´ìš©ì‹œê°„     ì£¼ì°¨ì‹œì„¤ ìœ ëª¨ì°¨ ëŒ€ì—¬ ì—¬ë¶€  \\\n",
       "10690  ...  NaN    NaN  NaN  NaN  09:00~16:00  ê°€ëŠ¥ (ë¬´ë£Œ)        ì—†ìŒ   \n",
       "\n",
       "      ì• ì™„ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥ ì—¬ë¶€ ì‹ ìš©ì¹´ë“œ ê°€ëŠ¥ ì—¬ë¶€  \\\n",
       "10690            ë¶ˆê°€        NaN   \n",
       "\n",
       "                                                    ìƒì„¸ì •ë³´  \n",
       "10690  ì´ìš©ê°€ëŠ¥ì‹œì„¤:ì „ë§ëŒ€ ì£¼ë³€ì˜ ê¸°ë…ë¹„ ë° ì°¸ì „íƒ‘, ì¥ë³‘ë“¤ì˜ ì¢…êµ ì§‘íšŒì¥ì†Œì¸ êµíšŒÂ·ì„±ë‹¹Â·...  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp['ëª…ì¹­'].str.contains('íƒœí’ì „ë§ëŒ€', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install unsloth\n",
    "# # Also get the latest nightly Unsloth!\n",
    "# %pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# # Install Flash Attention 2 for softcapping support\n",
    "# import torch\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     %pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: OpenAI failed to import - ignoring for now.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth: yanolja/EEVE-Korean-2.8B-v1.0 not supported yet!\nMake an issue to https://github.com/unslothai/unsloth!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 4ë°° ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì›í•˜ëŠ” 4bit ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m fourbit_models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-instruct-v0.2-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/llama-3-8b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Llama-3 8B\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ]  \u001b[38;5;66;03m# ë” ë§ì€ ëª¨ë¸ì€ https://huggingface.co/unsloth ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name = \"unsloth/llama-3-8b-bnb-4bit\", # ê³µì‹ íŠœí† ë¦¬ì–¼\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"unsloth/gemma-2-9b\",  # ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"yanolja/EEVE-Korean-10.8B-v1.0\",\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myanolja/EEVE-Korean-2.8B-v1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 4bit ì–‘ìí™” ë¡œë“œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token = \"hf_...\", # ê²Œì´íŠ¸ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: meta-llama/Llama-2-7b-hf\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yanolja/lib/python3.11/site-packages/unsloth/models/loader.py:334\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m     dispatch_model \u001b[38;5;241m=\u001b[39m FastCohereModel\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported yet!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake an issue to https://github.com/unslothai/unsloth!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Check if this is local model since the tokenizer gets overwritten\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: yanolja/EEVE-Korean-2.8B-v1.0 not supported yet!\nMake an issue to https://github.com/unslothai/unsloth!"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ RoPE ìŠ¤ì¼€ì¼ë§ì„ ìë™ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤!\n",
    "# max_seq_length = 4096  # ê³µì‹ íŠœí† ë¦¬ì–¼ì€ 2048\n",
    "max_seq_length = 512\n",
    "# ìë™ ê°ì§€ë¥¼ ìœ„í•´ Noneì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Tesla T4, V100ì€ Float16, Ampere+ëŠ” Bfloat16ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "dtype = None\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Falseì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4ë°° ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì›í•˜ëŠ” 4bit ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7bì˜ Instruct ë²„ì „\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2bì˜ Instruct ë²„ì „\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
    "]  # ë” ë§ì€ ëª¨ë¸ì€ https://huggingface.co/unsloth ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/llama-3-8b-bnb-4bit\", # ê³µì‹ íŠœí† ë¦¬ì–¼\n",
    "    # model_name=\"unsloth/gemma-2-9b\",  # ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # model_name=\"yanolja/EEVE-Korean-10.8B-v1.0\",\n",
    "    model_name=\"yanolja/EEVE-Korean-2.8B-v1.0\",\n",
    "    max_seq_length=max_seq_length,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    dtype=dtype,  # ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    load_in_4bit=load_in_4bit,  # 4bit ì–‘ìí™” ë¡œë“œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # token = \"hf_...\", # ê²Œì´íŠ¸ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='yanolja/EEVE-Korean-10.8B-v1.0', vocab_size=40960, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.01.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.11.5 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 4, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.01, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# {}\n",
    "\n",
    "# ### Input:\n",
    "# {}\n",
    "\n",
    "# ### Response:\n",
    "# {}\"\"\"\n",
    "\n",
    "# EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "# def formatting_prompts_func(examples):\n",
    "#     instructions = examples[\"instruction\"]\n",
    "#     inputs       = examples[\"input\"]\n",
    "#     outputs      = examples[\"output\"]\n",
    "#     texts = []\n",
    "#     for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "#         texts.append(text)\n",
    "#     return { \"text\" : texts, }\n",
    "# pass\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# # dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "# # dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_template = \"\"\"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ê³¼ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­:\n",
    "{instruction}\n",
    "\n",
    "### ì…ë ¥:\n",
    "{input}\n",
    "\n",
    "### ì‘ë‹µ:\"\"\"\n",
    "\n",
    "\n",
    "prompt_no_input_template = \"\"\"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­:\n",
    "{instruction}\n",
    "\n",
    "### ì‘ë‹µ:\"\"\"\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    instruction = data_point[\"instruction\"]\n",
    "    input = data_point[\"input\"]\n",
    "    label = data_point[\"output\"]\n",
    "\n",
    "    if input:\n",
    "        res = prompt_input_template.format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = prompt_no_input_template.format(instruction=instruction)\n",
    "\n",
    "    if label:\n",
    "        res = f\"{res}{label}<|im_end|>\" # eos_tokenì„ ë§ˆì§€ë§‰ì— ì¶”ê°€\n",
    "\n",
    "    data_point['text'] = res\n",
    "\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c67da80b0f4466fa8cc08a1974a7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/hwang/projects/yanolja/src/modeling/data/instruction_dataset_all_rows_all_scenarios_gpt35_tourAPI_tourist_attractions_seoul_gyeonggi_2.4K_scenario.jsonl\"\n",
    "\n",
    "json_data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        json_data.append(json.loads(line.strip()))\n",
    "\n",
    "dataset2 = pd.DataFrame(json_data)\n",
    "\n",
    "for col in dataset2.columns:\n",
    "    dataset2[col] = dataset2[col].apply(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x)\n",
    "\n",
    "dataset_hf = Dataset.from_pandas(dataset2)\n",
    "\n",
    "def combine_columns(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output = example[\"output\"]\n",
    "\n",
    "    combined_text = (\n",
    "        f\"Instruction:\\n{instruction}\\n\\n\"\n",
    "        f\"Input:\\n{input_text}\\n\\n\"\n",
    "        f\"Response:\\n{output}</s>\"\n",
    "    )\n",
    "\n",
    "    example[\"text\"] = combined_text\n",
    "    return example\n",
    "\n",
    "dataset_cvted = dataset_hf.map(combine_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9322ffa6da9d4078b7de92cacd2d9271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "  outputs = tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "  return outputs\n",
    "\n",
    "# remove_column_keys = dataset_cvted.features.keys()\n",
    "dataset_tokenized = dataset_cvted.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    examples_batch = tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "    examples_batch['labels'] = examples_batch['input_ids'] # ëª¨ë¸ í•™ìŠµ í‰ê°€ë¥¼ ìœ„í•œ loss ê³„ì‚°ì„ ìœ„í•´ ì…ë ¥ í† í°ì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©\n",
    "    return examples_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_split_size_mb:128\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token\n\u001b[1;32m     12\u001b[0m eval_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "eval_step = 20\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_tokenized,\n",
    "    # data_collator = collate_fn,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 512,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs= 2,\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 1,\n",
    "        # max_steps = 500, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        # fp16 = True,\n",
    "        # bf16 = False,\n",
    "        logging_steps = 20,\n",
    "        # optim = \"adamw_8bit\",\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=eval_step,\n",
    "        save_total_limit=2,\n",
    "        weight_decay = 0.01,\n",
    "        # lr_scheduler_type = \"linear\",\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.527 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52a2c10f97042ceb0dcd1519c3a7dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 4ë°° ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì›í•˜ëŠ” 4bit ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m fourbit_models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-instruct-v0.2-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/llama-3-8b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Llama-3 8B\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ]  \u001b[38;5;66;03m# ë” ë§ì€ ëª¨ë¸ì€ https://huggingface.co/unsloth ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name = \"unsloth/llama-3-8b-bnb-4bit\", # ê³µì‹ íŠœí† ë¦¬ì–¼\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"unsloth/gemma-2-9b\",  # ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"yanolja/EEVE-Korean-10.8B-v1.0\",\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/jaesung/yanolja/test_code/outputs/model_EEVE_10_8B_qa_dataset_introduction_EEVE_0.5K/checkpoint-387\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 4bit ì–‘ìí™” ë¡œë“œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token = \"hf_...\", # ê²Œì´íŠ¸ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: meta-llama/Llama-2-7b-hf\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/site-packages/unsloth/models/loader.py:332\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/site-packages/unsloth/models/llama.py:1790\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m Trainer\u001b[38;5;241m.\u001b[39m_inner_training_loop \u001b[38;5;241m=\u001b[39m _fast_inner_training_loop\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;66;03m# Fix gradient accumulation\u001b[39;00m\n\u001b[0;32m-> 1790\u001b[0m \u001b[43mpatch_gradient_accumulation_fix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;66;03m# Save tokenizer for inference purposes\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Force inference\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/site-packages/unsloth/models/_utils.py:1219\u001b[0m, in \u001b[0;36mpatch_gradient_accumulation_fix\u001b[0;34m(Trainer)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# Also fix up loss scaling ie negate loss *= self.args.gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(Trainer\u001b[38;5;241m.\u001b[39mtraining_step)\u001b[38;5;241m.\u001b[39mparameters: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m where \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1221\u001b[0m function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/inspect.py:1282\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/inspect.py:1264\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;124;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 1264\u001b[0m lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mtb_frame\n",
      "File \u001b[0;32m~/anaconda3/envs/meal/lib/python3.12/inspect.py:1093\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not get source code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: could not get source code"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ RoPE ìŠ¤ì¼€ì¼ë§ì„ ìë™ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤!\n",
    "# max_seq_length = 4096  # ê³µì‹ íŠœí† ë¦¬ì–¼ì€ 2048\n",
    "max_seq_length = 512\n",
    "# ìë™ ê°ì§€ë¥¼ ìœ„í•´ Noneì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Tesla T4, V100ì€ Float16, Ampere+ëŠ” Bfloat16ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "dtype = None\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Falseì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4ë°° ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì›í•˜ëŠ” 4bit ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7bì˜ Instruct ë²„ì „\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2bì˜ Instruct ë²„ì „\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
    "]  # ë” ë§ì€ ëª¨ë¸ì€ https://huggingface.co/unsloth ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/llama-3-8b-bnb-4bit\", # ê³µì‹ íŠœí† ë¦¬ì–¼\n",
    "    # model_name=\"unsloth/gemma-2-9b\",  # ëª¨ë¸ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # model_name=\"yanolja/EEVE-Korean-10.8B-v1.0\",\n",
    "    model_name=\"/home/hwang/projects/yanolja/src/modeling/outputs/checkpoint-225\",\n",
    "    max_seq_length=max_seq_length,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    dtype=dtype,  # ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    load_in_4bit=load_in_4bit,  # 4bit ì–‘ìí™” ë¡œë“œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # token = \"hf_...\", # ê²Œì´íŠ¸ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241m.\u001b[39mfor_inference(model) \u001b[38;5;66;03m# Enable native 2x faster inference\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# alpaca_prompt = You MUST copy from above!\u001b[39;00m\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      6\u001b[0m [\n\u001b[1;32m      7\u001b[0m     prompt_input_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m ], return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt_input_template.format(\n",
    "        instruction = \"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë‹µë³€ì€ ì œê³µëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ì¶”ê°€ ì •ë³´ê°€ ì—†ìœ¼ë©´ ëª…í™•íˆ ì•Œë¦¬ì„¸ìš”.\", # instruction\n",
    "        # input = \"ëŒ€êµ¬ì—ì„œ ê°€ë³¼ë§Œ í•œ ìˆ˜ëª©ì› ì•Œë ¤ì¤˜.\", # input\n",
    "        input = \"ë‚´ë§˜ëŒ€ë¡œí°ì¼€ì´ìŠ¤ í™ëŒ€ì ì˜ ì´ìš©ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\"\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë‚´ë§˜ëŒ€ë¡œí°ì¼€ì´ìŠ¤ í™ëŒ€ì ì€ 12:00ë¶€í„° 22:00ê¹Œì§€ ìš´ì˜ë©ë‹ˆë‹¤.<|im_end|>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].split(\"### ì‘ë‹µ:\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## í•™ìŠµí•œ ë°ì´í„° ì–‘ê³¼ ì§ˆì´ ë¶€ì¡±í•¨. ë‹¹ì—°íˆ Alignì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŒ. 2ì°¨ë…„ë„ë¶€í„° ê°œì„ í•´ ë‚˜ê°ˆ ì˜ˆì •."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yanolja_kernel",
   "language": "python",
   "name": "yanolja"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
